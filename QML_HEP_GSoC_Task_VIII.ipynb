{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7922722,"sourceType":"datasetVersion","datasetId":4656028}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-24T12:47:36.477362Z","iopub.execute_input":"2024-03-24T12:47:36.478202Z","iopub.status.idle":"2024-03-24T12:47:44.293551Z","shell.execute_reply.started":"2024-03-24T12:47:36.478162Z","shell.execute_reply":"2024-03-24T12:47:44.292654Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install flax","metadata":{"execution":{"iopub.status.busy":"2024-03-24T13:42:24.963634Z","iopub.execute_input":"2024-03-24T13:42:24.964029Z","iopub.status.idle":"2024-03-24T13:42:37.601447Z","shell.execute_reply.started":"2024-03-24T13:42:24.963993Z","shell.execute_reply":"2024-03-24T13:42:37.600217Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Requirement already satisfied: flax in /opt/conda/lib/python3.10/site-packages (0.8.2)\nRequirement already satisfied: numpy>=1.22 in /opt/conda/lib/python3.10/site-packages (from flax) (1.26.4)\nRequirement already satisfied: jax>=0.4.19 in /opt/conda/lib/python3.10/site-packages (from flax) (0.4.23)\nRequirement already satisfied: msgpack in /opt/conda/lib/python3.10/site-packages (from flax) (1.0.7)\nRequirement already satisfied: optax in /opt/conda/lib/python3.10/site-packages (from flax) (0.2.1)\nRequirement already satisfied: orbax-checkpoint in /opt/conda/lib/python3.10/site-packages (from flax) (0.5.6)\nRequirement already satisfied: tensorstore in /opt/conda/lib/python3.10/site-packages (from flax) (0.1.56)\nRequirement already satisfied: rich>=11.1 in /opt/conda/lib/python3.10/site-packages (from flax) (13.7.0)\nRequirement already satisfied: typing-extensions>=4.2 in /opt/conda/lib/python3.10/site-packages (from flax) (4.9.0)\nRequirement already satisfied: PyYAML>=5.4.1 in /opt/conda/lib/python3.10/site-packages (from flax) (6.0.1)\nRequirement already satisfied: ml-dtypes>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from jax>=0.4.19->flax) (0.2.0)\nRequirement already satisfied: opt-einsum in /opt/conda/lib/python3.10/site-packages (from jax>=0.4.19->flax) (3.3.0)\nRequirement already satisfied: scipy>=1.9 in /opt/conda/lib/python3.10/site-packages (from jax>=0.4.19->flax) (1.11.4)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1->flax) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1->flax) (2.17.2)\nRequirement already satisfied: absl-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from optax->flax) (1.4.0)\nRequirement already satisfied: chex>=0.1.7 in /opt/conda/lib/python3.10/site-packages (from optax->flax) (0.1.85)\nRequirement already satisfied: jaxlib>=0.1.37 in /opt/conda/lib/python3.10/site-packages (from optax->flax) (0.4.23.dev20240116)\nRequirement already satisfied: etils[epath,epy] in /opt/conda/lib/python3.10/site-packages (from orbax-checkpoint->flax) (1.6.0)\nRequirement already satisfied: nest_asyncio in /opt/conda/lib/python3.10/site-packages (from orbax-checkpoint->flax) (1.5.8)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from orbax-checkpoint->flax) (3.20.3)\nCollecting ml-dtypes>=0.2.0 (from jax>=0.4.19->flax)\n  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\nRequirement already satisfied: toolz>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from chex>=0.1.7->optax->flax) (0.12.1)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax) (0.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from etils[epath,epy]->orbax-checkpoint->flax) (2024.3.0)\nRequirement already satisfied: importlib_resources in /opt/conda/lib/python3.10/site-packages (from etils[epath,epy]->orbax-checkpoint->flax) (6.1.1)\nRequirement already satisfied: zipp in /opt/conda/lib/python3.10/site-packages (from etils[epath,epy]->orbax-checkpoint->flax) (3.17.0)\nDownloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: ml-dtypes\n  Attempting uninstall: ml-dtypes\n    Found existing installation: ml-dtypes 0.2.0\n    Uninstalling ml-dtypes-0.2.0:\n      Successfully uninstalled ml-dtypes-0.2.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.0.5 which is incompatible.\ntensorflow 2.15.0 requires ml-dtypes~=0.2.0, but you have ml-dtypes 0.3.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed ml-dtypes-0.3.2\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install jax","metadata":{"execution":{"iopub.status.busy":"2024-03-24T13:42:05.877187Z","iopub.execute_input":"2024-03-24T13:42:05.878014Z","iopub.status.idle":"2024-03-24T13:42:17.936707Z","shell.execute_reply.started":"2024-03-24T13:42:05.877975Z","shell.execute_reply":"2024-03-24T13:42:17.935511Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Requirement already satisfied: jax in /opt/conda/lib/python3.10/site-packages (0.4.23)\nRequirement already satisfied: ml-dtypes>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from jax) (0.2.0)\nRequirement already satisfied: numpy>=1.22 in /opt/conda/lib/python3.10/site-packages (from jax) (1.26.4)\nRequirement already satisfied: opt-einsum in /opt/conda/lib/python3.10/site-packages (from jax) (3.3.0)\nRequirement already satisfied: scipy>=1.9 in /opt/conda/lib/python3.10/site-packages (from jax) (1.11.4)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install tensorcircuit","metadata":{"execution":{"iopub.status.busy":"2024-03-24T13:41:36.307247Z","iopub.execute_input":"2024-03-24T13:41:36.308091Z","iopub.status.idle":"2024-03-24T13:41:49.605304Z","shell.execute_reply.started":"2024-03-24T13:41:36.308057Z","shell.execute_reply":"2024-03-24T13:41:49.604175Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Collecting tensorcircuit\n  Downloading tensorcircuit-0.12.0-py3-none-any.whl.metadata (29 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from tensorcircuit) (1.26.4)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from tensorcircuit) (1.11.4)\nCollecting tensornetwork-ng (from tensorcircuit)\n  Downloading tensornetwork_ng-0.5.0-py3-none-any.whl.metadata (7.0 kB)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from tensorcircuit) (3.2.1)\nRequirement already satisfied: graphviz>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from tensornetwork-ng->tensorcircuit) (0.20.2)\nRequirement already satisfied: opt-einsum>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from tensornetwork-ng->tensorcircuit) (3.3.0)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensornetwork-ng->tensorcircuit) (3.10.0)\nDownloading tensorcircuit-0.12.0-py3-none-any.whl (342 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m342.0/342.0 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading tensornetwork_ng-0.5.0-py3-none-any.whl (243 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.3/243.3 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: tensornetwork-ng, tensorcircuit\nSuccessfully installed tensorcircuit-0.12.0 tensornetwork-ng-0.5.0\n","output_type":"stream"}]},{"cell_type":"code","source":"DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-03-24T12:47:44.295249Z","iopub.execute_input":"2024-03-24T12:47:44.295792Z","iopub.status.idle":"2024-03-24T12:47:44.320686Z","shell.execute_reply.started":"2024-03-24T12:47:44.295759Z","shell.execute_reply":"2024-03-24T12:47:44.319664Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2024-03-24T12:49:23.458113Z","iopub.execute_input":"2024-03-24T12:49:23.458789Z","iopub.status.idle":"2024-03-24T12:49:23.467916Z","shell.execute_reply.started":"2024-03-24T12:49:23.458750Z","shell.execute_reply":"2024-03-24T12:49:23.466974Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/kaggle/input/digit-recognizer/sample_submission.csv\n/kaggle/input/digit-recognizer/train.csv\n/kaggle/input/digit-recognizer/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"class MNISTDataset(Dataset):\n    def __init__(self, data_df:pd.DataFrame, transform=None, is_test=False):\n        super(MNISTDataset, self).__init__()\n        dataset = []\n            \n        for i, row in tqdm(data_df.iterrows(), total=len(data_df)):\n            data = row.to_numpy()\n            if is_test:\n                label = -1\n                image = data.reshape(28, 28).astype(np.uint8)\n            else:\n                label = data[0]\n                image = data[1:].reshape(28, 28).astype(np.uint8)\n            \n            if transform is not None:\n                image = transform(image)\n                    \n            dataset.append((image, label))\n        self.dataset = dataset\n        self.transform = transform\n        self.is_test = is_test\n    \n    def __len__(self):\n        return len(self.dataset)\n    \n    def __getitem__(self, i):\n        return self.dataset[i]","metadata":{"execution":{"iopub.status.busy":"2024-03-24T12:49:24.146998Z","iopub.execute_input":"2024-03-24T12:49:24.147716Z","iopub.status.idle":"2024-03-24T12:49:24.157065Z","shell.execute_reply.started":"2024-03-24T12:49:24.147680Z","shell.execute_reply":"2024-03-24T12:49:24.155288Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"data_train = pd.read_csv('/kaggle/input/digit-recognizer/train.csv')\ntrain_data = data_train.drop('label', axis=1).values\ntrain_mean = train_data.mean()/255.\ntrain_std = train_data.std()/255.\neval_count = 1000\ntrain_transform = transform=transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.RandomRotation(15),\n    transforms.RandomAffine(degrees=20, translate=(0.1,0.1), scale=(0.9, 1.1)),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[train_mean], std=[train_std]),\n])\nval_transform = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[train_mean], std=[train_std]),\n])\ntest_transform = val_transform\n# train_dataset = MNISTDataset(data_train.iloc[:-eval_count], default_transform)\ntrain_dataset = MNISTDataset(data_train, train_transform) # use this to train the model on the full training set\neval_dataset = MNISTDataset(data_train.iloc[-eval_count:], val_transform)","metadata":{"execution":{"iopub.status.busy":"2024-03-24T12:49:25.037326Z","iopub.execute_input":"2024-03-24T12:49:25.037725Z","iopub.status.idle":"2024-03-24T12:50:06.269364Z","shell.execute_reply.started":"2024-03-24T12:49:25.037699Z","shell.execute_reply":"2024-03-24T12:50:06.268468Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/42000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef77af1fc8be41c390ed5aaf7eeb7d36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07e7b8ee24bd4e24bb5c3198c88dad4a"}},"metadata":{}}]},{"cell_type":"code","source":"data_test = pd.read_csv('/kaggle/input/digit-recognizer/test.csv')\ntest_dataset = MNISTDataset(data_test, test_transform, is_test=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-24T12:50:06.271071Z","iopub.execute_input":"2024-03-24T12:50:06.271361Z","iopub.status.idle":"2024-03-24T12:50:17.948372Z","shell.execute_reply.started":"2024-03-24T12:50:06.271336Z","shell.execute_reply":"2024-03-24T12:50:17.947420Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/28000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a79a78a367484356954c06c9ece4fd08"}},"metadata":{}}]},{"cell_type":"code","source":"data_train","metadata":{"execution":{"iopub.status.busy":"2024-03-24T12:54:30.927503Z","iopub.execute_input":"2024-03-24T12:54:30.927868Z","iopub.status.idle":"2024-03-24T12:54:30.956302Z","shell.execute_reply.started":"2024-03-24T12:54:30.927842Z","shell.execute_reply":"2024-03-24T12:54:30.955441Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"       label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n0          1       0       0       0       0       0       0       0       0   \n1          0       0       0       0       0       0       0       0       0   \n2          1       0       0       0       0       0       0       0       0   \n3          4       0       0       0       0       0       0       0       0   \n4          0       0       0       0       0       0       0       0       0   \n...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n41995      0       0       0       0       0       0       0       0       0   \n41996      1       0       0       0       0       0       0       0       0   \n41997      7       0       0       0       0       0       0       0       0   \n41998      6       0       0       0       0       0       0       0       0   \n41999      9       0       0       0       0       0       0       0       0   \n\n       pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  \\\n0           0  ...         0         0         0         0         0   \n1           0  ...         0         0         0         0         0   \n2           0  ...         0         0         0         0         0   \n3           0  ...         0         0         0         0         0   \n4           0  ...         0         0         0         0         0   \n...       ...  ...       ...       ...       ...       ...       ...   \n41995       0  ...         0         0         0         0         0   \n41996       0  ...         0         0         0         0         0   \n41997       0  ...         0         0         0         0         0   \n41998       0  ...         0         0         0         0         0   \n41999       0  ...         0         0         0         0         0   \n\n       pixel779  pixel780  pixel781  pixel782  pixel783  \n0             0         0         0         0         0  \n1             0         0         0         0         0  \n2             0         0         0         0         0  \n3             0         0         0         0         0  \n4             0         0         0         0         0  \n...         ...       ...       ...       ...       ...  \n41995         0         0         0         0         0  \n41996         0         0         0         0         0  \n41997         0         0         0         0         0  \n41998         0         0         0         0         0  \n41999         0         0         0         0         0  \n\n[42000 rows x 785 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>pixel0</th>\n      <th>pixel1</th>\n      <th>pixel2</th>\n      <th>pixel3</th>\n      <th>pixel4</th>\n      <th>pixel5</th>\n      <th>pixel6</th>\n      <th>pixel7</th>\n      <th>pixel8</th>\n      <th>...</th>\n      <th>pixel774</th>\n      <th>pixel775</th>\n      <th>pixel776</th>\n      <th>pixel777</th>\n      <th>pixel778</th>\n      <th>pixel779</th>\n      <th>pixel780</th>\n      <th>pixel781</th>\n      <th>pixel782</th>\n      <th>pixel783</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>41995</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>41996</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>41997</th>\n      <td>7</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>41998</th>\n      <td>6</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>41999</th>\n      <td>9</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>42000 rows × 785 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data_train.label.unique()","metadata":{"execution":{"iopub.status.busy":"2024-03-24T12:54:31.917080Z","iopub.execute_input":"2024-03-24T12:54:31.917463Z","iopub.status.idle":"2024-03-24T12:54:31.927816Z","shell.execute_reply.started":"2024-03-24T12:54:31.917430Z","shell.execute_reply":"2024-03-24T12:54:31.926825Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"array([1, 0, 4, 7, 3, 5, 8, 9, 2, 6])"},"metadata":{}}]},{"cell_type":"code","source":"data_train[data_train.label == 1].to_numpy()","metadata":{"execution":{"iopub.status.busy":"2024-03-24T12:54:32.717153Z","iopub.execute_input":"2024-03-24T12:54:32.717529Z","iopub.status.idle":"2024-03-24T12:54:32.753747Z","shell.execute_reply.started":"2024-03-24T12:54:32.717499Z","shell.execute_reply":"2024-03-24T12:54:32.752812Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"array([[1, 0, 0, ..., 0, 0, 0],\n       [1, 0, 0, ..., 0, 0, 0],\n       [1, 0, 0, ..., 0, 0, 0],\n       ...,\n       [1, 0, 0, ..., 0, 0, 0],\n       [1, 0, 0, ..., 0, 0, 0],\n       [1, 0, 0, ..., 0, 0, 0]])"},"metadata":{}}]},{"cell_type":"code","source":"row = data_train.iloc[1].to_numpy()\nlabel = row[0]\ndigit_img = row[1:].reshape(28, 28)\nprint(\"label:\",label)","metadata":{"execution":{"iopub.status.busy":"2024-03-24T12:54:33.397119Z","iopub.execute_input":"2024-03-24T12:54:33.397494Z","iopub.status.idle":"2024-03-24T12:54:33.403454Z","shell.execute_reply.started":"2024-03-24T12:54:33.397464Z","shell.execute_reply":"2024-03-24T12:54:33.402448Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"label: 0\n","output_type":"stream"}]},{"cell_type":"code","source":"plt.imshow(digit_img, interpolation='nearest', cmap='gray')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-24T12:54:34.146585Z","iopub.execute_input":"2024-03-24T12:54:34.146930Z","iopub.status.idle":"2024-03-24T12:54:34.391407Z","shell.execute_reply.started":"2024-03-24T12:54:34.146903Z","shell.execute_reply":"2024-03-24T12:54:34.390552Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"!pip install einops","metadata":{"execution":{"iopub.status.busy":"2024-03-24T12:54:34.876886Z","iopub.execute_input":"2024-03-24T12:54:34.877746Z","iopub.status.idle":"2024-03-24T12:54:48.068511Z","shell.execute_reply.started":"2024-03-24T12:54:34.877715Z","shell.execute_reply":"2024-03-24T12:54:48.067364Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Collecting einops\n  Downloading einops-0.7.0-py3-none-any.whl.metadata (13 kB)\nDownloading einops-0.7.0-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: einops\nSuccessfully installed einops-0.7.0\n","output_type":"stream"}]},{"cell_type":"code","source":"from einops import rearrange","metadata":{"execution":{"iopub.status.busy":"2024-03-24T12:54:48.071226Z","iopub.execute_input":"2024-03-24T12:54:48.071710Z","iopub.status.idle":"2024-03-24T12:54:48.086228Z","shell.execute_reply.started":"2024-03-24T12:54:48.071664Z","shell.execute_reply":"2024-03-24T12:54:48.085420Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class Residual(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n\n    def forward(self, x, **kwargs):\n        return self.fn(x, **kwargs) + x\n\nclass PreNorm(nn.Module):\n    def __init__(self, dim, fn):\n        super().__init__()\n        self.norm = nn.LayerNorm(dim)\n        self.fn = fn\n\n    def forward(self, x, **kwargs):\n        return self.fn(self.norm(x), **kwargs)\n\nclass FeedForward(nn.Module):\n    def __init__(self, dim, hidden_dim):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(dim, hidden_dim),\n            nn.GELU(),\n            nn.Linear(hidden_dim, dim)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass Attention(nn.Module):\n    def __init__(self, dim, heads=8):\n        super().__init__()\n        self.heads = heads\n        self.scale = dim ** -0.5\n\n        self.to_qkv = nn.Linear(dim, dim * 3, bias=False)\n        self.to_out = nn.Linear(dim, dim)\n\n    def forward(self, x, mask = None):\n        b, n, _, h = *x.shape, self.heads\n        qkv = self.to_qkv(x)\n        q, k, v = rearrange(qkv, 'b n (qkv h d) -> qkv b h n d', qkv=3, h=h)\n\n        dots = torch.einsum('bhid,bhjd->bhij', q, k) * self.scale\n\n        if mask is not None:\n            mask = F.pad(mask.flatten(1), (1, 0), value = True)\n            assert mask.shape[-1] == dots.shape[-1], 'mask has incorrect dimensions'\n            mask = mask[:, None, :] * mask[:, :, None]\n            dots.masked_fill_(~mask, float('-inf'))\n            del mask\n\n        attn = dots.softmax(dim=-1)\n\n        out = torch.einsum('bhij,bhjd->bhid', attn, v)\n        out = rearrange(out, 'b h n d -> b n (h d)')\n        out =  self.to_out(out)\n        return out\n\nclass Transformer(nn.Module):\n    def __init__(self, dim, depth, heads, mlp_dim):\n        super().__init__()\n        self.layers = nn.ModuleList([])\n        for _ in range(depth):\n            self.layers.append(nn.ModuleList([\n                Residual(PreNorm(dim, Attention(dim, heads = heads))),\n                Residual(PreNorm(dim, FeedForward(dim, mlp_dim)))\n            ]))\n\n    def forward(self, x, mask=None):\n        for attn, ff in self.layers:\n            x = attn(x, mask=mask)\n            x = ff(x)\n        return x\n    \nclass ViT(nn.Module):\n    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, channels=3):\n        super().__init__()\n        assert image_size % patch_size == 0, 'image dimensions must be divisible by the patch size'\n        num_patches = (image_size // patch_size) ** 2\n        patch_dim = channels * patch_size ** 2\n\n        self.patch_size = patch_size\n\n        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n        self.patch_to_embedding = nn.Linear(patch_dim, dim)\n        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n        self.transformer = Transformer(dim, depth, heads, mlp_dim)\n\n        self.to_cls_token = nn.Identity()\n\n        self.mlp_head = nn.Sequential(\n            nn.Linear(dim, mlp_dim),\n            nn.GELU(),\n            nn.Linear(mlp_dim, num_classes)\n        )\n\n    def forward(self, img, mask=None):\n        p = self.patch_size\n\n        x = rearrange(img, 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=p, p2=p)\n        x = self.patch_to_embedding(x)\n\n        cls_tokens = self.cls_token.expand(img.shape[0], -1, -1)\n        x = torch.cat((cls_tokens, x), dim=1)\n        x += self.pos_embedding\n        x = self.transformer(x, mask)\n\n        x = self.to_cls_token(x[:, 0])\n        return self.mlp_head(x)","metadata":{"execution":{"iopub.status.busy":"2024-03-24T12:54:52.881365Z","iopub.execute_input":"2024-03-24T12:54:52.881768Z","iopub.status.idle":"2024-03-24T12:54:52.908695Z","shell.execute_reply.started":"2024-03-24T12:54:52.881738Z","shell.execute_reply":"2024-03-24T12:54:52.907701Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"torch.manual_seed(42)\n\nBATCH_SIZE_TRAIN = 100\nBATCH_SIZE_TEST = 1000\n\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE_TRAIN, shuffle=True)\neval_loader = torch.utils.data.DataLoader(eval_dataset, batch_size=BATCH_SIZE_TEST, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-03-24T12:54:54.097191Z","iopub.execute_input":"2024-03-24T12:54:54.097575Z","iopub.status.idle":"2024-03-24T12:54:54.106286Z","shell.execute_reply.started":"2024-03-24T12:54:54.097545Z","shell.execute_reply":"2024-03-24T12:54:54.105326Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"import torch.nn.functional as F\n\ndef train_epoch(model, optimizer, data_loader, loss_history):\n    total_samples = len(data_loader.dataset)\n    model.train()\n\n    for i, (data, target) in enumerate(data_loader):\n        data = data.to(DEVICE)\n        target = target.to(DEVICE)\n        optimizer.zero_grad()\n        output = F.log_softmax(model(data), dim=1)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n        optimizer.step()\n\n        if i % 100 == 0:\n            print('[' +  '{:5}'.format(i * len(data)) + '/' + '{:5}'.format(total_samples) +\n                  ' (' + '{:3.0f}'.format(100 * i / len(data_loader)) + '%)]  Loss: ' +\n                  '{:6.4f}'.format(loss.item()))\n            loss_history.append(loss.item())","metadata":{"execution":{"iopub.status.busy":"2024-03-24T12:54:54.781976Z","iopub.execute_input":"2024-03-24T12:54:54.782417Z","iopub.status.idle":"2024-03-24T12:54:54.790022Z","shell.execute_reply.started":"2024-03-24T12:54:54.782373Z","shell.execute_reply":"2024-03-24T12:54:54.789126Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def evaluate(model, data_loader, loss_history):\n    model.eval()\n    \n    total_samples = len(data_loader.dataset)\n    correct_samples = 0\n    total_loss = 0\n\n    with torch.no_grad():\n        for data, target in data_loader:\n            data = data.to(DEVICE)\n            target = target.to(DEVICE)\n            output = F.log_softmax(model(data), dim=1)\n            loss = F.nll_loss(output, target, reduction='sum')\n            _, pred = torch.max(output, dim=1)\n            \n            total_loss += loss.item()\n            correct_samples += pred.eq(target).sum()\n\n    avg_loss = total_loss / total_samples\n    loss_history.append(avg_loss)\n    print('\\nAverage test loss: ' + '{:.4f}'.format(avg_loss) +\n          '  Accuracy:' + '{:5}'.format(correct_samples) + '/' +\n          '{:5}'.format(total_samples) + ' (' +\n          '{:4.2f}'.format(100.0 * correct_samples / total_samples) + '%)\\n')","metadata":{"execution":{"iopub.status.busy":"2024-03-24T12:54:55.501917Z","iopub.execute_input":"2024-03-24T12:54:55.502532Z","iopub.status.idle":"2024-03-24T12:54:55.510165Z","shell.execute_reply.started":"2024-03-24T12:54:55.502500Z","shell.execute_reply":"2024-03-24T12:54:55.509159Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"import time\n\nmodel = ViT(image_size=28, patch_size=7, num_classes=10, channels=1,\n            dim=64, depth=6, heads=8, mlp_dim=128)\nmodel = model.to(DEVICE)\n# optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\noptimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n\ntrain_loss_history, test_loss_history = [], []","metadata":{"execution":{"iopub.status.busy":"2024-03-24T12:54:56.301857Z","iopub.execute_input":"2024-03-24T12:54:56.302469Z","iopub.status.idle":"2024-03-24T12:54:56.499509Z","shell.execute_reply.started":"2024-03-24T12:54:56.302438Z","shell.execute_reply":"2024-03-24T12:54:56.498734Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"N_EPOCHS = 100\n\n# lambda1 = lambda epoch: epoch // 30\n# lambda2 = lambda epoch: 0.95 ** epoch\n# scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda2)\nscheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n\nstart_time = time.time()\nfor epoch in range(1, N_EPOCHS + 1):\n    print('Epoch:', epoch,'LR:', scheduler.get_last_lr())\n    train_epoch(model, optimizer, train_loader, train_loss_history)\n    evaluate(model, eval_loader, test_loss_history)\n    scheduler.step()\n\nprint('Execution time:', '{:5.2f}'.format(time.time() - start_time), 'seconds')","metadata":{"execution":{"iopub.status.busy":"2024-03-24T12:54:57.377281Z","iopub.execute_input":"2024-03-24T12:54:57.378124Z","iopub.status.idle":"2024-03-24T13:03:37.187900Z","shell.execute_reply.started":"2024-03-24T12:54:57.378091Z","shell.execute_reply":"2024-03-24T13:03:37.186970Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Epoch: 1 LR: [0.1]\n[    0/42000 (  0%)]  Loss: 2.3242\n[10000/42000 ( 24%)]  Loss: 2.1623\n[20000/42000 ( 48%)]  Loss: 1.6863\n[30000/42000 ( 71%)]  Loss: 1.4569\n[40000/42000 ( 95%)]  Loss: 0.8872\n\nAverage test loss: 0.5540  Accuracy:  808/ 1000 (80.80%)\n\nEpoch: 2 LR: [0.095]\n[    0/42000 (  0%)]  Loss: 0.9230\n[10000/42000 ( 24%)]  Loss: 0.6968\n[20000/42000 ( 48%)]  Loss: 0.8771\n[30000/42000 ( 71%)]  Loss: 0.5411\n[40000/42000 ( 95%)]  Loss: 0.3563\n\nAverage test loss: 0.2184  Accuracy:  938/ 1000 (93.80%)\n\nEpoch: 3 LR: [0.09025]\n[    0/42000 (  0%)]  Loss: 0.5534\n[10000/42000 ( 24%)]  Loss: 0.3451\n[20000/42000 ( 48%)]  Loss: 0.2632\n[30000/42000 ( 71%)]  Loss: 0.3104\n[40000/42000 ( 95%)]  Loss: 0.4901\n\nAverage test loss: 0.1444  Accuracy:  953/ 1000 (95.30%)\n\nEpoch: 4 LR: [0.0857375]\n[    0/42000 (  0%)]  Loss: 0.4852\n[10000/42000 ( 24%)]  Loss: 0.2459\n[20000/42000 ( 48%)]  Loss: 0.5079\n[30000/42000 ( 71%)]  Loss: 0.1935\n[40000/42000 ( 95%)]  Loss: 0.2357\n\nAverage test loss: 0.1624  Accuracy:  953/ 1000 (95.30%)\n\nEpoch: 5 LR: [0.08145062499999998]\n[    0/42000 (  0%)]  Loss: 0.2042\n[10000/42000 ( 24%)]  Loss: 0.3853\n[20000/42000 ( 48%)]  Loss: 0.4812\n[30000/42000 ( 71%)]  Loss: 0.1746\n[40000/42000 ( 95%)]  Loss: 0.0787\n\nAverage test loss: 0.1153  Accuracy:  965/ 1000 (96.50%)\n\nEpoch: 6 LR: [0.07737809374999999]\n[    0/42000 (  0%)]  Loss: 0.3276\n[10000/42000 ( 24%)]  Loss: 0.2420\n[20000/42000 ( 48%)]  Loss: 0.0846\n[30000/42000 ( 71%)]  Loss: 0.2917\n[40000/42000 ( 95%)]  Loss: 0.2210\n\nAverage test loss: 0.1073  Accuracy:  969/ 1000 (96.90%)\n\nEpoch: 7 LR: [0.07350918906249998]\n[    0/42000 (  0%)]  Loss: 0.1348\n[10000/42000 ( 24%)]  Loss: 0.0937\n[20000/42000 ( 48%)]  Loss: 0.1580\n[30000/42000 ( 71%)]  Loss: 0.2081\n[40000/42000 ( 95%)]  Loss: 0.1852\n\nAverage test loss: 0.1121  Accuracy:  963/ 1000 (96.30%)\n\nEpoch: 8 LR: [0.06983372960937498]\n[    0/42000 (  0%)]  Loss: 0.1053\n[10000/42000 ( 24%)]  Loss: 0.1353\n[20000/42000 ( 48%)]  Loss: 0.2489\n[30000/42000 ( 71%)]  Loss: 0.2175\n[40000/42000 ( 95%)]  Loss: 0.1598\n\nAverage test loss: 0.1017  Accuracy:  965/ 1000 (96.50%)\n\nEpoch: 9 LR: [0.06634204312890622]\n[    0/42000 (  0%)]  Loss: 0.1522\n[10000/42000 ( 24%)]  Loss: 0.3205\n[20000/42000 ( 48%)]  Loss: 0.1160\n[30000/42000 ( 71%)]  Loss: 0.0871\n[40000/42000 ( 95%)]  Loss: 0.1769\n\nAverage test loss: 0.1154  Accuracy:  969/ 1000 (96.90%)\n\nEpoch: 10 LR: [0.0630249409724609]\n[    0/42000 (  0%)]  Loss: 0.1307\n[10000/42000 ( 24%)]  Loss: 0.1145\n[20000/42000 ( 48%)]  Loss: 0.2036\n[30000/42000 ( 71%)]  Loss: 0.1138\n[40000/42000 ( 95%)]  Loss: 0.1199\n\nAverage test loss: 0.0871  Accuracy:  976/ 1000 (97.60%)\n\nEpoch: 11 LR: [0.05987369392383786]\n[    0/42000 (  0%)]  Loss: 0.1795\n[10000/42000 ( 24%)]  Loss: 0.0650\n[20000/42000 ( 48%)]  Loss: 0.1282\n[30000/42000 ( 71%)]  Loss: 0.0485\n[40000/42000 ( 95%)]  Loss: 0.0862\n\nAverage test loss: 0.0783  Accuracy:  978/ 1000 (97.80%)\n\nEpoch: 12 LR: [0.05688000922764597]\n[    0/42000 (  0%)]  Loss: 0.1656\n[10000/42000 ( 24%)]  Loss: 0.0595\n[20000/42000 ( 48%)]  Loss: 0.0171\n[30000/42000 ( 71%)]  Loss: 0.0871\n[40000/42000 ( 95%)]  Loss: 0.1085\n\nAverage test loss: 0.0675  Accuracy:  976/ 1000 (97.60%)\n\nEpoch: 13 LR: [0.05403600876626367]\n[    0/42000 (  0%)]  Loss: 0.1174\n[10000/42000 ( 24%)]  Loss: 0.0297\n[20000/42000 ( 48%)]  Loss: 0.1203\n[30000/42000 ( 71%)]  Loss: 0.1153\n[40000/42000 ( 95%)]  Loss: 0.1617\n\nAverage test loss: 0.1015  Accuracy:  977/ 1000 (97.70%)\n\nEpoch: 14 LR: [0.05133420832795048]\n[    0/42000 (  0%)]  Loss: 0.0731\n[10000/42000 ( 24%)]  Loss: 0.0043\n[20000/42000 ( 48%)]  Loss: 0.0903\n[30000/42000 ( 71%)]  Loss: 0.0458\n[40000/42000 ( 95%)]  Loss: 0.1666\n\nAverage test loss: 0.1276  Accuracy:  977/ 1000 (97.70%)\n\nEpoch: 15 LR: [0.04876749791155295]\n[    0/42000 (  0%)]  Loss: 0.0908\n[10000/42000 ( 24%)]  Loss: 0.0520\n[20000/42000 ( 48%)]  Loss: 0.0319\n[30000/42000 ( 71%)]  Loss: 0.0546\n[40000/42000 ( 95%)]  Loss: 0.1101\n\nAverage test loss: 0.0747  Accuracy:  978/ 1000 (97.80%)\n\nEpoch: 16 LR: [0.046329123015975304]\n[    0/42000 (  0%)]  Loss: 0.0509\n[10000/42000 ( 24%)]  Loss: 0.1116\n[20000/42000 ( 48%)]  Loss: 0.0968\n[30000/42000 ( 71%)]  Loss: 0.0347\n[40000/42000 ( 95%)]  Loss: 0.0289\n\nAverage test loss: 0.0653  Accuracy:  985/ 1000 (98.50%)\n\nEpoch: 17 LR: [0.04401266686517654]\n[    0/42000 (  0%)]  Loss: 0.0335\n[10000/42000 ( 24%)]  Loss: 0.0361\n[20000/42000 ( 48%)]  Loss: 0.0311\n[30000/42000 ( 71%)]  Loss: 0.0208\n[40000/42000 ( 95%)]  Loss: 0.0390\n\nAverage test loss: 0.0949  Accuracy:  975/ 1000 (97.50%)\n\nEpoch: 18 LR: [0.04181203352191771]\n[    0/42000 (  0%)]  Loss: 0.0073\n[10000/42000 ( 24%)]  Loss: 0.0142\n[20000/42000 ( 48%)]  Loss: 0.0199\n[30000/42000 ( 71%)]  Loss: 0.0283\n[40000/42000 ( 95%)]  Loss: 0.0134\n\nAverage test loss: 0.0936  Accuracy:  984/ 1000 (98.40%)\n\nEpoch: 19 LR: [0.039721431845821824]\n[    0/42000 (  0%)]  Loss: 0.0105\n[10000/42000 ( 24%)]  Loss: 0.0017\n[20000/42000 ( 48%)]  Loss: 0.0217\n[30000/42000 ( 71%)]  Loss: 0.0063\n[40000/42000 ( 95%)]  Loss: 0.0046\n\nAverage test loss: 0.0682  Accuracy:  986/ 1000 (98.60%)\n\nEpoch: 20 LR: [0.037735360253530734]\n[    0/42000 (  0%)]  Loss: 0.0181\n[10000/42000 ( 24%)]  Loss: 0.0001\n[20000/42000 ( 48%)]  Loss: 0.0046\n[30000/42000 ( 71%)]  Loss: 0.0006\n[40000/42000 ( 95%)]  Loss: 0.0011\n\nAverage test loss: 0.1561  Accuracy:  974/ 1000 (97.40%)\n\nEpoch: 21 LR: [0.035848592240854196]\n[    0/42000 (  0%)]  Loss: 0.0284\n[10000/42000 ( 24%)]  Loss: 0.0093\n[20000/42000 ( 48%)]  Loss: 0.0004\n[30000/42000 ( 71%)]  Loss: 0.0014\n[40000/42000 ( 95%)]  Loss: 0.0076\n\nAverage test loss: 0.0782  Accuracy:  985/ 1000 (98.50%)\n\nEpoch: 22 LR: [0.03405616262881148]\n[    0/42000 (  0%)]  Loss: 0.0013\n[10000/42000 ( 24%)]  Loss: 0.0026\n[20000/42000 ( 48%)]  Loss: 0.0002\n[30000/42000 ( 71%)]  Loss: 0.0014\n[40000/42000 ( 95%)]  Loss: 0.0003\n\nAverage test loss: 0.0755  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 23 LR: [0.03235335449737091]\n[    0/42000 (  0%)]  Loss: 0.0006\n[10000/42000 ( 24%)]  Loss: 0.0003\n[20000/42000 ( 48%)]  Loss: 0.0003\n[30000/42000 ( 71%)]  Loss: 0.0001\n[40000/42000 ( 95%)]  Loss: 0.0009\n\nAverage test loss: 0.0758  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 24 LR: [0.030735686772502362]\n[    0/42000 (  0%)]  Loss: 0.0004\n[10000/42000 ( 24%)]  Loss: 0.0006\n[20000/42000 ( 48%)]  Loss: 0.0000\n[30000/42000 ( 71%)]  Loss: 0.0002\n[40000/42000 ( 95%)]  Loss: 0.0001\n\nAverage test loss: 0.0822  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 25 LR: [0.029198902433877242]\n[    0/42000 (  0%)]  Loss: 0.0001\n[10000/42000 ( 24%)]  Loss: 0.0001\n[20000/42000 ( 48%)]  Loss: 0.0000\n[30000/42000 ( 71%)]  Loss: 0.0001\n[40000/42000 ( 95%)]  Loss: 0.0001\n\nAverage test loss: 0.0821  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 26 LR: [0.027738957312183378]\n[    0/42000 (  0%)]  Loss: 0.0000\n[10000/42000 ( 24%)]  Loss: 0.0001\n[20000/42000 ( 48%)]  Loss: 0.0000\n[30000/42000 ( 71%)]  Loss: 0.0001\n[40000/42000 ( 95%)]  Loss: 0.0001\n\nAverage test loss: 0.0829  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 27 LR: [0.026352009446574207]\n[    0/42000 (  0%)]  Loss: 0.0001\n[10000/42000 ( 24%)]  Loss: 0.0000\n[20000/42000 ( 48%)]  Loss: 0.0001\n[30000/42000 ( 71%)]  Loss: 0.0000\n[40000/42000 ( 95%)]  Loss: 0.0001\n\nAverage test loss: 0.0835  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 28 LR: [0.025034408974245494]\n[    0/42000 (  0%)]  Loss: 0.0000\n[10000/42000 ( 24%)]  Loss: 0.0001\n[20000/42000 ( 48%)]  Loss: 0.0002\n[30000/42000 ( 71%)]  Loss: 0.0002\n[40000/42000 ( 95%)]  Loss: 0.0000\n\nAverage test loss: 0.0838  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 29 LR: [0.023782688525533217]\n[    0/42000 (  0%)]  Loss: 0.0001\n[10000/42000 ( 24%)]  Loss: 0.0000\n[20000/42000 ( 48%)]  Loss: 0.0001\n[30000/42000 ( 71%)]  Loss: 0.0000\n[40000/42000 ( 95%)]  Loss: 0.0000\n\nAverage test loss: 0.0844  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 30 LR: [0.022593554099256556]\n[    0/42000 (  0%)]  Loss: 0.0001\n[10000/42000 ( 24%)]  Loss: 0.0001\n[20000/42000 ( 48%)]  Loss: 0.0001\n[30000/42000 ( 71%)]  Loss: 0.0000\n[40000/42000 ( 95%)]  Loss: 0.0001\n\nAverage test loss: 0.0846  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 31 LR: [0.021463876394293726]\n[    0/42000 (  0%)]  Loss: 0.0001\n[10000/42000 ( 24%)]  Loss: 0.0002\n[20000/42000 ( 48%)]  Loss: 0.0002\n[30000/42000 ( 71%)]  Loss: 0.0000\n[40000/42000 ( 95%)]  Loss: 0.0001\n\nAverage test loss: 0.0849  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 32 LR: [0.020390682574579037]\n[    0/42000 (  0%)]  Loss: 0.0003\n[10000/42000 ( 24%)]  Loss: 0.0002\n[20000/42000 ( 48%)]  Loss: 0.0000\n[30000/42000 ( 71%)]  Loss: 0.0000\n[40000/42000 ( 95%)]  Loss: 0.0001\n\nAverage test loss: 0.0853  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 33 LR: [0.019371148445850084]\n[    0/42000 (  0%)]  Loss: 0.0000\n[10000/42000 ( 24%)]  Loss: 0.0000\n[20000/42000 ( 48%)]  Loss: 0.0000\n[30000/42000 ( 71%)]  Loss: 0.0000\n[40000/42000 ( 95%)]  Loss: 0.0000\n\nAverage test loss: 0.0856  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 34 LR: [0.01840259102355758]\n[    0/42000 (  0%)]  Loss: 0.0000\n[10000/42000 ( 24%)]  Loss: 0.0001\n[20000/42000 ( 48%)]  Loss: 0.0001\n[30000/42000 ( 71%)]  Loss: 0.0000\n[40000/42000 ( 95%)]  Loss: 0.0002\n\nAverage test loss: 0.0857  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 35 LR: [0.0174824614723797]\n[    0/42000 (  0%)]  Loss: 0.0001\n[10000/42000 ( 24%)]  Loss: 0.0000\n[20000/42000 ( 48%)]  Loss: 0.0001\n[30000/42000 ( 71%)]  Loss: 0.0001\n[40000/42000 ( 95%)]  Loss: 0.0002\n\nAverage test loss: 0.0860  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 36 LR: [0.016608338398760712]\n[    0/42000 (  0%)]  Loss: 0.0000\n[10000/42000 ( 24%)]  Loss: 0.0001\n[20000/42000 ( 48%)]  Loss: 0.0000\n[30000/42000 ( 71%)]  Loss: 0.0002\n[40000/42000 ( 95%)]  Loss: 0.0000\n\nAverage test loss: 0.0862  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 37 LR: [0.015777921478822676]\n[    0/42000 (  0%)]  Loss: 0.0000\n[10000/42000 ( 24%)]  Loss: 0.0001\n[20000/42000 ( 48%)]  Loss: 0.0001\n[30000/42000 ( 71%)]  Loss: 0.0002\n[40000/42000 ( 95%)]  Loss: 0.0001\n\nAverage test loss: 0.0864  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 38 LR: [0.014989025404881541]\n[    0/42000 (  0%)]  Loss: 0.0000\n[10000/42000 ( 24%)]  Loss: 0.0001\n[20000/42000 ( 48%)]  Loss: 0.0001\n[30000/42000 ( 71%)]  Loss: 0.0001\n[40000/42000 ( 95%)]  Loss: 0.0000\n\nAverage test loss: 0.0866  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 39 LR: [0.014239574134637464]\n[    0/42000 (  0%)]  Loss: 0.0000\n[10000/42000 ( 24%)]  Loss: 0.0000\n[20000/42000 ( 48%)]  Loss: 0.0000\n[30000/42000 ( 71%)]  Loss: 0.0000\n[40000/42000 ( 95%)]  Loss: 0.0001\n\nAverage test loss: 0.0867  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 40 LR: [0.01352759542790559]\n[    0/42000 (  0%)]  Loss: 0.0000\n[10000/42000 ( 24%)]  Loss: 0.0000\n[20000/42000 ( 48%)]  Loss: 0.0001\n[30000/42000 ( 71%)]  Loss: 0.0000\n[40000/42000 ( 95%)]  Loss: 0.0000\n\nAverage test loss: 0.0868  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 41 LR: [0.012851215656510309]\n[    0/42000 (  0%)]  Loss: 0.0001\n[10000/42000 ( 24%)]  Loss: 0.0000\n[20000/42000 ( 48%)]  Loss: 0.0000\n[30000/42000 ( 71%)]  Loss: 0.0004\n[40000/42000 ( 95%)]  Loss: 0.0000\n\nAverage test loss: 0.0870  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 42 LR: [0.012208654873684792]\n[    0/42000 (  0%)]  Loss: 0.0001\n[10000/42000 ( 24%)]  Loss: 0.0001\n[20000/42000 ( 48%)]  Loss: 0.0001\n[30000/42000 ( 71%)]  Loss: 0.0000\n[40000/42000 ( 95%)]  Loss: 0.0000\n\nAverage test loss: 0.0871  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 43 LR: [0.011598222130000552]\n[    0/42000 (  0%)]  Loss: 0.0000\n[10000/42000 ( 24%)]  Loss: 0.0000\n[20000/42000 ( 48%)]  Loss: 0.0000\n[30000/42000 ( 71%)]  Loss: 0.0000\n[40000/42000 ( 95%)]  Loss: 0.0000\n\nAverage test loss: 0.0872  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 44 LR: [0.011018311023500524]\n[    0/42000 (  0%)]  Loss: 0.0000\n[10000/42000 ( 24%)]  Loss: 0.0000\n[20000/42000 ( 48%)]  Loss: 0.0000\n[30000/42000 ( 71%)]  Loss: 0.0000\n[40000/42000 ( 95%)]  Loss: 0.0002\n\nAverage test loss: 0.0873  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 45 LR: [0.010467395472325497]\n[    0/42000 (  0%)]  Loss: 0.0000\n[10000/42000 ( 24%)]  Loss: 0.0001\n[20000/42000 ( 48%)]  Loss: 0.0000\n[30000/42000 ( 71%)]  Loss: 0.0001\n[40000/42000 ( 95%)]  Loss: 0.0001\n\nAverage test loss: 0.0874  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 46 LR: [0.009944025698709221]\n[    0/42000 (  0%)]  Loss: 0.0002\n[10000/42000 ( 24%)]  Loss: 0.0001\n[20000/42000 ( 48%)]  Loss: 0.0000\n[30000/42000 ( 71%)]  Loss: 0.0000\n[40000/42000 ( 95%)]  Loss: 0.0001\n\nAverage test loss: 0.0875  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 47 LR: [0.00944682441377376]\n[    0/42000 (  0%)]  Loss: 0.0001\n[10000/42000 ( 24%)]  Loss: 0.0000\n[20000/42000 ( 48%)]  Loss: 0.0001\n[30000/42000 ( 71%)]  Loss: 0.0001\n[40000/42000 ( 95%)]  Loss: 0.0002\n\nAverage test loss: 0.0875  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 48 LR: [0.00897448319308507]\n[    0/42000 (  0%)]  Loss: 0.0002\n[10000/42000 ( 24%)]  Loss: 0.0000\n[20000/42000 ( 48%)]  Loss: 0.0002\n[30000/42000 ( 71%)]  Loss: 0.0001\n[40000/42000 ( 95%)]  Loss: 0.0000\n\nAverage test loss: 0.0876  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 49 LR: [0.008525759033430816]\n[    0/42000 (  0%)]  Loss: 0.0000\n[10000/42000 ( 24%)]  Loss: 0.0000\n[20000/42000 ( 48%)]  Loss: 0.0000\n[30000/42000 ( 71%)]  Loss: 0.0001\n[40000/42000 ( 95%)]  Loss: 0.0000\n\nAverage test loss: 0.0877  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 50 LR: [0.008099471081759275]\n[    0/42000 (  0%)]  Loss: 0.0001\n[10000/42000 ( 24%)]  Loss: 0.0000\n[20000/42000 ( 48%)]  Loss: 0.0000\n[30000/42000 ( 71%)]  Loss: 0.0000\n[40000/42000 ( 95%)]  Loss: 0.0000\n\nAverage test loss: 0.0877  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 51 LR: [0.007694497527671311]\n[    0/42000 (  0%)]  Loss: 0.0000\n[10000/42000 ( 24%)]  Loss: 0.0001\n[20000/42000 ( 48%)]  Loss: 0.0001\n[30000/42000 ( 71%)]  Loss: 0.0001\n[40000/42000 ( 95%)]  Loss: 0.0001\n\nAverage test loss: 0.0878  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 52 LR: [0.007309772651287745]\n[    0/42000 (  0%)]  Loss: 0.0000\n[10000/42000 ( 24%)]  Loss: 0.0001\n[20000/42000 ( 48%)]  Loss: 0.0001\n[30000/42000 ( 71%)]  Loss: 0.0000\n[40000/42000 ( 95%)]  Loss: 0.0000\n\nAverage test loss: 0.0879  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 53 LR: [0.006944284018723357]\n[    0/42000 (  0%)]  Loss: 0.0002\n[10000/42000 ( 24%)]  Loss: 0.0000\n[20000/42000 ( 48%)]  Loss: 0.0000\n[30000/42000 ( 71%)]  Loss: 0.0001\n[40000/42000 ( 95%)]  Loss: 0.0001\n\nAverage test loss: 0.0880  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 54 LR: [0.006597069817787189]\n[    0/42000 (  0%)]  Loss: 0.0001\n[10000/42000 ( 24%)]  Loss: 0.0001\n[20000/42000 ( 48%)]  Loss: 0.0000\n[30000/42000 ( 71%)]  Loss: 0.0001\n[40000/42000 ( 95%)]  Loss: 0.0000\n\nAverage test loss: 0.0880  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 55 LR: [0.006267216326897829]\n[    0/42000 (  0%)]  Loss: 0.0000\n[10000/42000 ( 24%)]  Loss: 0.0001\n[20000/42000 ( 48%)]  Loss: 0.0001\n[30000/42000 ( 71%)]  Loss: 0.0000\n[40000/42000 ( 95%)]  Loss: 0.0001\n\nAverage test loss: 0.0881  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 56 LR: [0.005953855510552938]\n[    0/42000 (  0%)]  Loss: 0.0000\n[10000/42000 ( 24%)]  Loss: 0.0000\n[20000/42000 ( 48%)]  Loss: 0.0000\n[30000/42000 ( 71%)]  Loss: 0.0001\n[40000/42000 ( 95%)]  Loss: 0.0001\n\nAverage test loss: 0.0881  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 57 LR: [0.005656162735025291]\n[    0/42000 (  0%)]  Loss: 0.0000\n[10000/42000 ( 24%)]  Loss: 0.0000\n[20000/42000 ( 48%)]  Loss: 0.0000\n[30000/42000 ( 71%)]  Loss: 0.0000\n[40000/42000 ( 95%)]  Loss: 0.0000\n\nAverage test loss: 0.0881  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 58 LR: [0.005373354598274026]\n[    0/42000 (  0%)]  Loss: 0.0002\n[10000/42000 ( 24%)]  Loss: 0.0000\n[20000/42000 ( 48%)]  Loss: 0.0001\n[30000/42000 ( 71%)]  Loss: 0.0000\n[40000/42000 ( 95%)]  Loss: 0.0001\n\nAverage test loss: 0.0882  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 59 LR: [0.005104686868360324]\n[    0/42000 (  0%)]  Loss: 0.0000\n[10000/42000 ( 24%)]  Loss: 0.0001\n[20000/42000 ( 48%)]  Loss: 0.0000\n[30000/42000 ( 71%)]  Loss: 0.0000\n[40000/42000 ( 95%)]  Loss: 0.0000\n\nAverage test loss: 0.0882  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 60 LR: [0.004849452524942308]\n[    0/42000 (  0%)]  Loss: 0.0000\n[10000/42000 ( 24%)]  Loss: 0.0000\n[20000/42000 ( 48%)]  Loss: 0.0001\n[30000/42000 ( 71%)]  Loss: 0.0001\n[40000/42000 ( 95%)]  Loss: 0.0000\n\nAverage test loss: 0.0883  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 61 LR: [0.004606979898695193]\n[    0/42000 (  0%)]  Loss: 0.0000\n[10000/42000 ( 24%)]  Loss: 0.0001\n[20000/42000 ( 48%)]  Loss: 0.0000\n[30000/42000 ( 71%)]  Loss: 0.0000\n[40000/42000 ( 95%)]  Loss: 0.0000\n\nAverage test loss: 0.0883  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 62 LR: [0.004376630903760433]\n[    0/42000 (  0%)]  Loss: 0.0000\n[10000/42000 ( 24%)]  Loss: 0.0000\n[20000/42000 ( 48%)]  Loss: 0.0000\n[30000/42000 ( 71%)]  Loss: 0.0000\n[40000/42000 ( 95%)]  Loss: 0.0000\n\nAverage test loss: 0.0883  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 63 LR: [0.0041577993585724116]\n[    0/42000 (  0%)]  Loss: 0.0000\n[10000/42000 ( 24%)]  Loss: 0.0000\n[20000/42000 ( 48%)]  Loss: 0.0000\n[30000/42000 ( 71%)]  Loss: 0.0000\n[40000/42000 ( 95%)]  Loss: 0.0000\n\nAverage test loss: 0.0884  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 64 LR: [0.0039499093906437905]\n[    0/42000 (  0%)]  Loss: 0.0001\n[10000/42000 ( 24%)]  Loss: 0.0000\n[20000/42000 ( 48%)]  Loss: 0.0000\n[30000/42000 ( 71%)]  Loss: 0.0000\n[40000/42000 ( 95%)]  Loss: 0.0000\n\nAverage test loss: 0.0884  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 65 LR: [0.0037524139211116006]\n[    0/42000 (  0%)]  Loss: 0.0000\n[10000/42000 ( 24%)]  Loss: 0.0000\n[20000/42000 ( 48%)]  Loss: 0.0000\n[30000/42000 ( 71%)]  Loss: 0.0001\n[40000/42000 ( 95%)]  Loss: 0.0000\n\nAverage test loss: 0.0884  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 66 LR: [0.0035647932250560204]\n[    0/42000 (  0%)]  Loss: 0.0000\n[10000/42000 ( 24%)]  Loss: 0.0000\n[20000/42000 ( 48%)]  Loss: 0.0000\n[30000/42000 ( 71%)]  Loss: 0.0000\n[40000/42000 ( 95%)]  Loss: 0.0000\n\nAverage test loss: 0.0885  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 67 LR: [0.003386553563803219]\n[    0/42000 (  0%)]  Loss: 0.0002\n[10000/42000 ( 24%)]  Loss: 0.0001\n[20000/42000 ( 48%)]  Loss: 0.0001\n[30000/42000 ( 71%)]  Loss: 0.0000\n[40000/42000 ( 95%)]  Loss: 0.0001\n\nAverage test loss: 0.0885  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 68 LR: [0.003217225885613058]\n[    0/42000 (  0%)]  Loss: 0.0000\n[10000/42000 ( 24%)]  Loss: 0.0000\n[20000/42000 ( 48%)]  Loss: 0.0000\n[30000/42000 ( 71%)]  Loss: 0.0001\n[40000/42000 ( 95%)]  Loss: 0.0000\n\nAverage test loss: 0.0885  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 69 LR: [0.0030563645913324047]\n[    0/42000 (  0%)]  Loss: 0.0001\n[10000/42000 ( 24%)]  Loss: 0.0000\n[20000/42000 ( 48%)]  Loss: 0.0001\n[30000/42000 ( 71%)]  Loss: 0.0000\n[40000/42000 ( 95%)]  Loss: 0.0001\n\nAverage test loss: 0.0885  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 70 LR: [0.0029035463617657843]\n[    0/42000 (  0%)]  Loss: 0.0000\n[10000/42000 ( 24%)]  Loss: 0.0000\n[20000/42000 ( 48%)]  Loss: 0.0000\n[30000/42000 ( 71%)]  Loss: 0.0000\n[40000/42000 ( 95%)]  Loss: 0.0001\n\nAverage test loss: 0.0886  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 71 LR: [0.002758369043677495]\n[    0/42000 (  0%)]  Loss: 0.0001\n[10000/42000 ( 24%)]  Loss: 0.0000\n[20000/42000 ( 48%)]  Loss: 0.0000\n[30000/42000 ( 71%)]  Loss: 0.0000\n[40000/42000 ( 95%)]  Loss: 0.0001\n\nAverage test loss: 0.0886  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 72 LR: [0.00262045059149362]\n[    0/42000 (  0%)]  Loss: 0.0000\n[10000/42000 ( 24%)]  Loss: 0.0001\n[20000/42000 ( 48%)]  Loss: 0.0000\n[30000/42000 ( 71%)]  Loss: 0.0000\n[40000/42000 ( 95%)]  Loss: 0.0000\n\nAverage test loss: 0.0886  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 73 LR: [0.002489428061918939]\n[    0/42000 (  0%)]  Loss: 0.0000\n[10000/42000 ( 24%)]  Loss: 0.0000\n[20000/42000 ( 48%)]  Loss: 0.0001\n[30000/42000 ( 71%)]  Loss: 0.0001\n[40000/42000 ( 95%)]  Loss: 0.0001\n\nAverage test loss: 0.0886  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 74 LR: [0.002364956658822992]\n[    0/42000 (  0%)]  Loss: 0.0000\n[10000/42000 ( 24%)]  Loss: 0.0001\n[20000/42000 ( 48%)]  Loss: 0.0001\n[30000/42000 ( 71%)]  Loss: 0.0001\n[40000/42000 ( 95%)]  Loss: 0.0000\n\nAverage test loss: 0.0886  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 75 LR: [0.002246708825881842]\n[    0/42000 (  0%)]  Loss: 0.0000\n[10000/42000 ( 24%)]  Loss: 0.0001\n[20000/42000 ( 48%)]  Loss: 0.0000\n[30000/42000 ( 71%)]  Loss: 0.0002\n[40000/42000 ( 95%)]  Loss: 0.0000\n\nAverage test loss: 0.0886  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 76 LR: [0.00213437338458775]\n[    0/42000 (  0%)]  Loss: 0.0002\n[10000/42000 ( 24%)]  Loss: 0.0000\n[20000/42000 ( 48%)]  Loss: 0.0000\n[30000/42000 ( 71%)]  Loss: 0.0000\n[40000/42000 ( 95%)]  Loss: 0.0000\n\nAverage test loss: 0.0887  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 77 LR: [0.0020276547153583622]\n[    0/42000 (  0%)]  Loss: 0.0000\n[10000/42000 ( 24%)]  Loss: 0.0001\n[20000/42000 ( 48%)]  Loss: 0.0000\n[30000/42000 ( 71%)]  Loss: 0.0000\n[40000/42000 ( 95%)]  Loss: 0.0000\n\nAverage test loss: 0.0887  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 78 LR: [0.001926271979590444]\n[    0/42000 (  0%)]  Loss: 0.0000\n[10000/42000 ( 24%)]  Loss: 0.0000\n[20000/42000 ( 48%)]  Loss: 0.0000\n[30000/42000 ( 71%)]  Loss: 0.0001\n[40000/42000 ( 95%)]  Loss: 0.0001\n\nAverage test loss: 0.0887  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 79 LR: [0.0018299583806109217]\n[    0/42000 (  0%)]  Loss: 0.0001\n[10000/42000 ( 24%)]  Loss: 0.0000\n[20000/42000 ( 48%)]  Loss: 0.0001\n[30000/42000 ( 71%)]  Loss: 0.0001\n[40000/42000 ( 95%)]  Loss: 0.0000\n\nAverage test loss: 0.0887  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 80 LR: [0.0017384604615803755]\n[    0/42000 (  0%)]  Loss: 0.0000\n[10000/42000 ( 24%)]  Loss: 0.0000\n[20000/42000 ( 48%)]  Loss: 0.0001\n[30000/42000 ( 71%)]  Loss: 0.0003\n[40000/42000 ( 95%)]  Loss: 0.0000\n\nAverage test loss: 0.0887  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 81 LR: [0.0016515374385013568]\n[    0/42000 (  0%)]  Loss: 0.0001\n[10000/42000 ( 24%)]  Loss: 0.0000\n[20000/42000 ( 48%)]  Loss: 0.0001\n[30000/42000 ( 71%)]  Loss: 0.0000\n[40000/42000 ( 95%)]  Loss: 0.0002\n\nAverage test loss: 0.0887  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 82 LR: [0.0015689605665762888]\n[    0/42000 (  0%)]  Loss: 0.0000\n[10000/42000 ( 24%)]  Loss: 0.0000\n[20000/42000 ( 48%)]  Loss: 0.0000\n[30000/42000 ( 71%)]  Loss: 0.0000\n[40000/42000 ( 95%)]  Loss: 0.0002\n\nAverage test loss: 0.0887  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 83 LR: [0.0014905125382474742]\n[    0/42000 (  0%)]  Loss: 0.0000\n[10000/42000 ( 24%)]  Loss: 0.0001\n[20000/42000 ( 48%)]  Loss: 0.0000\n[30000/42000 ( 71%)]  Loss: 0.0001\n[40000/42000 ( 95%)]  Loss: 0.0000\n\nAverage test loss: 0.0888  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 84 LR: [0.0014159869113351004]\n[    0/42000 (  0%)]  Loss: 0.0001\n[10000/42000 ( 24%)]  Loss: 0.0000\n[20000/42000 ( 48%)]  Loss: 0.0000\n[30000/42000 ( 71%)]  Loss: 0.0000\n[40000/42000 ( 95%)]  Loss: 0.0000\n\nAverage test loss: 0.0888  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 85 LR: [0.0013451875657683454]\n[    0/42000 (  0%)]  Loss: 0.0000\n[10000/42000 ( 24%)]  Loss: 0.0000\n[20000/42000 ( 48%)]  Loss: 0.0000\n[30000/42000 ( 71%)]  Loss: 0.0001\n[40000/42000 ( 95%)]  Loss: 0.0000\n\nAverage test loss: 0.0888  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 86 LR: [0.001277928187479928]\n[    0/42000 (  0%)]  Loss: 0.0001\n[10000/42000 ( 24%)]  Loss: 0.0000\n[20000/42000 ( 48%)]  Loss: 0.0000\n[30000/42000 ( 71%)]  Loss: 0.0001\n[40000/42000 ( 95%)]  Loss: 0.0000\n\nAverage test loss: 0.0888  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 87 LR: [0.0012140317781059316]\n[    0/42000 (  0%)]  Loss: 0.0000\n[10000/42000 ( 24%)]  Loss: 0.0000\n[20000/42000 ( 48%)]  Loss: 0.0000\n[30000/42000 ( 71%)]  Loss: 0.0000\n[40000/42000 ( 95%)]  Loss: 0.0000\n\nAverage test loss: 0.0888  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 88 LR: [0.001153330189200635]\n[    0/42000 (  0%)]  Loss: 0.0000\n[10000/42000 ( 24%)]  Loss: 0.0000\n[20000/42000 ( 48%)]  Loss: 0.0000\n[30000/42000 ( 71%)]  Loss: 0.0000\n[40000/42000 ( 95%)]  Loss: 0.0000\n\nAverage test loss: 0.0888  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 89 LR: [0.0010956636797406032]\n[    0/42000 (  0%)]  Loss: 0.0000\n[10000/42000 ( 24%)]  Loss: 0.0000\n[20000/42000 ( 48%)]  Loss: 0.0000\n[30000/42000 ( 71%)]  Loss: 0.0000\n[40000/42000 ( 95%)]  Loss: 0.0000\n\nAverage test loss: 0.0888  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 90 LR: [0.001040880495753573]\n[    0/42000 (  0%)]  Loss: 0.0001\n[10000/42000 ( 24%)]  Loss: 0.0000\n[20000/42000 ( 48%)]  Loss: 0.0000\n[30000/42000 ( 71%)]  Loss: 0.0000\n[40000/42000 ( 95%)]  Loss: 0.0000\n\nAverage test loss: 0.0888  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 91 LR: [0.0009888364709658944]\n[    0/42000 (  0%)]  Loss: 0.0000\n[10000/42000 ( 24%)]  Loss: 0.0000\n[20000/42000 ( 48%)]  Loss: 0.0000\n[30000/42000 ( 71%)]  Loss: 0.0000\n[40000/42000 ( 95%)]  Loss: 0.0001\n\nAverage test loss: 0.0888  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 92 LR: [0.0009393946474175996]\n[    0/42000 (  0%)]  Loss: 0.0000\n[10000/42000 ( 24%)]  Loss: 0.0000\n[20000/42000 ( 48%)]  Loss: 0.0001\n[30000/42000 ( 71%)]  Loss: 0.0000\n[40000/42000 ( 95%)]  Loss: 0.0000\n\nAverage test loss: 0.0888  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 93 LR: [0.0008924249150467197]\n[    0/42000 (  0%)]  Loss: 0.0000\n[10000/42000 ( 24%)]  Loss: 0.0000\n[20000/42000 ( 48%)]  Loss: 0.0000\n[30000/42000 ( 71%)]  Loss: 0.0000\n[40000/42000 ( 95%)]  Loss: 0.0001\n\nAverage test loss: 0.0888  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 94 LR: [0.0008478036692943836]\n[    0/42000 (  0%)]  Loss: 0.0000\n[10000/42000 ( 24%)]  Loss: 0.0000\n[20000/42000 ( 48%)]  Loss: 0.0000\n[30000/42000 ( 71%)]  Loss: 0.0000\n[40000/42000 ( 95%)]  Loss: 0.0000\n\nAverage test loss: 0.0888  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 95 LR: [0.0008054134858296644]\n[    0/42000 (  0%)]  Loss: 0.0000\n[10000/42000 ( 24%)]  Loss: 0.0000\n[20000/42000 ( 48%)]  Loss: 0.0000\n[30000/42000 ( 71%)]  Loss: 0.0000\n[40000/42000 ( 95%)]  Loss: 0.0000\n\nAverage test loss: 0.0888  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 96 LR: [0.0007651428115381812]\n[    0/42000 (  0%)]  Loss: 0.0001\n[10000/42000 ( 24%)]  Loss: 0.0000\n[20000/42000 ( 48%)]  Loss: 0.0000\n[30000/42000 ( 71%)]  Loss: 0.0000\n[40000/42000 ( 95%)]  Loss: 0.0000\n\nAverage test loss: 0.0888  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 97 LR: [0.000726885670961272]\n[    0/42000 (  0%)]  Loss: 0.0000\n[10000/42000 ( 24%)]  Loss: 0.0000\n[20000/42000 ( 48%)]  Loss: 0.0000\n[30000/42000 ( 71%)]  Loss: 0.0001\n[40000/42000 ( 95%)]  Loss: 0.0000\n\nAverage test loss: 0.0889  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 98 LR: [0.0006905413874132084]\n[    0/42000 (  0%)]  Loss: 0.0000\n[10000/42000 ( 24%)]  Loss: 0.0000\n[20000/42000 ( 48%)]  Loss: 0.0001\n[30000/42000 ( 71%)]  Loss: 0.0001\n[40000/42000 ( 95%)]  Loss: 0.0000\n\nAverage test loss: 0.0889  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 99 LR: [0.0006560143180425479]\n[    0/42000 (  0%)]  Loss: 0.0000\n[10000/42000 ( 24%)]  Loss: 0.0000\n[20000/42000 ( 48%)]  Loss: 0.0000\n[30000/42000 ( 71%)]  Loss: 0.0000\n[40000/42000 ( 95%)]  Loss: 0.0000\n\nAverage test loss: 0.0889  Accuracy:  988/ 1000 (98.80%)\n\nEpoch: 100 LR: [0.0006232136021404205]\n[    0/42000 (  0%)]  Loss: 0.0000\n[10000/42000 ( 24%)]  Loss: 0.0000\n[20000/42000 ( 48%)]  Loss: 0.0000\n[30000/42000 ( 71%)]  Loss: 0.0000\n[40000/42000 ( 95%)]  Loss: 0.0000\n\nAverage test loss: 0.0889  Accuracy:  988/ 1000 (98.80%)\n\nExecution time: 519.80 seconds\n","output_type":"stream"}]},{"cell_type":"code","source":"submission_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE_TEST, shuffle=False)\n\n# Making it submission ready\nsubmission = [['ImageId', 'Label']]\n\n# Turn off gradients for validation\nwith torch.no_grad():\n    model.eval()\n    image_id = 1\n    \n    for images, _ in submission_loader:\n        images = images.to(DEVICE)\n        output = F.log_softmax(model(images), dim=1)\n        _, pred = torch.max(output, dim=1)\n        \n        for prediction in pred:\n            #print([image_id, prediction.item()])\n            submission.append([image_id, prediction.item()])\n            image_id += 1","metadata":{"execution":{"iopub.status.busy":"2024-03-24T13:05:21.608894Z","iopub.execute_input":"2024-03-24T13:05:21.609568Z","iopub.status.idle":"2024-03-24T13:05:22.408015Z","shell.execute_reply.started":"2024-03-24T13:05:21.609536Z","shell.execute_reply":"2024-03-24T13:05:22.407176Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"submission_df = pd.DataFrame(submission)\nsubmission_df.columns = submission_df.iloc[0]\nsubmission_df = submission_df.drop(0, axis=0)\nsubmission_df","metadata":{"execution":{"iopub.status.busy":"2024-03-24T13:05:24.367248Z","iopub.execute_input":"2024-03-24T13:05:24.367615Z","iopub.status.idle":"2024-03-24T13:05:24.390192Z","shell.execute_reply.started":"2024-03-24T13:05:24.367587Z","shell.execute_reply":"2024-03-24T13:05:24.388980Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"0     ImageId Label\n1           1     2\n2           2     0\n3           3     9\n4           4     9\n5           5     3\n...       ...   ...\n27996   27996     9\n27997   27997     7\n27998   27998     3\n27999   27999     9\n28000   28000     2\n\n[28000 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ImageId</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>27996</th>\n      <td>27996</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>27997</th>\n      <td>27997</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>27998</th>\n      <td>27998</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>27999</th>\n      <td>27999</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>28000</th>\n      <td>28000</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>28000 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"submission_df.to_csv('/kaggle/working/submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-03-24T13:05:25.657066Z","iopub.execute_input":"2024-03-24T13:05:25.657475Z","iopub.status.idle":"2024-03-24T13:05:25.700011Z","shell.execute_reply.started":"2024-03-24T13:05:25.657445Z","shell.execute_reply":"2024-03-24T13:05:25.699105Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"We can extend the classical vision transformer to a quantum vision transformer by making the following modification to the classical vision transformer architecture:\n\n1. Leverage quantum circuits in the model architecture\n2. Introduce quantum attention mechanisms \n3. Design novel methods for loading data and implementing trainable quantum orthogonal layers","metadata":{}},{"cell_type":"markdown","source":"## Sketch of The Quantum Vision Transformer Architecture\n\nQuantum vision transformers involve the following in their architecture:\n\n1. Quantum Data Loading : Quantum Vision Transformers load patches of data into quantum states using parametrized quantum circuits for efficient data representation\n\n2. Quantum Attention Mechanism : Quantum Vision Transformers introduce quantum attention mechanisms based on compound matrices, offering a theoretical advantage over classical attention mechanisms in terms of runtime and model parameters\n\n3. Trainable Quantum Orthogonal Layers: The architecture includes trainable orthogonal layers that can adapt to different levels of connectivity and the quality of quantum computers, enhancing the flexibility and the performance of the quantum vision transformers\n\n4. Simulation and performance : Extensive simulations on medical image datasets demonstrate competitive or superior performance compared to classical benchmarks, showing the effective of quantum computers in image analysis tasks\n\n5. Compound Transformer: A novel approach within the Quantum Vision Transformer architecture is the Compound Transformer, which loads all image patches in superposition and applies attention using an orthogonal matrix with trainable weights, showcasing a more natively quantum method for image analysis tasks\n\n6. Quantum Circuit Design: Quantum circuits in the Quantum Vision Transformer architecture involve loading matrices as quantum states, applying quantum orthogonal layers, and utilizing specific amplitude encoding and hamming-weight preserving quantum gates for efficient computation","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}